{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\asus\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import string\n",
    "import collections\n",
    "from nltk import word_tokenize\n",
    "from nltk.stem import PorterStemmer\n",
    "from nltk.corpus import stopwords\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from pprint import pprint\n",
    "import re\n",
    "import os\n",
    "from glob import glob\n",
    "from tqdm import tqdm\n",
    "from Sastrawi.StopWordRemover.StopWordRemoverFactory import StopWordRemoverFactory\n",
    "from Sastrawi.Stemmer.StemmerFactory import StemmerFactory\n",
    "import numpy as np\n",
    "import nltk\n",
    "nltk.download('punkt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocessing(data):\n",
    "    lowertext = data.lower()\n",
    "    replacetxt = re.sub('[^ a-zA-Z0-9]',' ',lowertext)\n",
    "    \n",
    "    fact = StopWordRemoverFactory()\n",
    "    stopword = fact.create_stop_word_remover()\n",
    "    stopwordRem = stopword.remove(str(replacetxt))\n",
    "    \n",
    "    token = nltk.word_tokenize(stopwordRem)\n",
    "\n",
    "    stemF = StemmerFactory()\n",
    "    stemmer = stemF.create_stemmer()\n",
    "    baseWord = [stemmer.stem(word) for word in token]\n",
    "    baseWord = ' '.join(baseWord)\n",
    "    \n",
    "    return baseWord"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "rss = []\n",
    "def cluster_texts(texts, clusters=3):\n",
    "    \"\"\" Transform texts to Tf-Idf coordinates and cluster texts using K-Means \"\"\"\n",
    "    vectorizer = TfidfVectorizer()\n",
    " \n",
    "    tfidf_model = vectorizer.fit_transform(texts)\n",
    "    km_model = KMeans(n_clusters=clusters, random_state=25)\n",
    "    km_model.fit(tfidf_model)\n",
    "    rss.append(km_model.inertia_)\n",
    " \n",
    "    clustering = collections.defaultdict(list)\n",
    " \n",
    "    for idx, label in enumerate(km_model.labels_):\n",
    "        clustering[label].append(idx)\n",
    " \n",
    "    return clustering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "base_path = \".\"\n",
    "data_path = os.path.join(base_path, \"data testing clustering\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "files = glob(data_path + \"/**/*.news\")\n",
    "inp_cluster = 6"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████████| 63/63 [03:38<00:00,  3.46s/it]\n"
     ]
    }
   ],
   "source": [
    "kategori = []\n",
    "documents = []\n",
    "for filename in tqdm(files):\n",
    "    f = open(filename, \"r\")\n",
    "    data = f.read()\n",
    "    f.close()\n",
    "    data = data.split('\\n')\n",
    "    tag = filename.split('\\\\')\n",
    "    tag = tag[2]\n",
    "    data = data[4]\n",
    "    data = preprocessing(data)\n",
    "    documents.append(data)\n",
    "    kategori.append(tag)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "kat = np.unique(kategori)\n",
    "c = {}\n",
    "for i in range(len(kat)):\n",
    "    c[kat[i]] = i"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{0: [26, 54, 55, 56, 58, 59, 61],\n",
      " 1: [10, 11, 12, 13, 14, 15, 16, 18, 20, 52],\n",
      " 2: [3, 4, 8, 9, 25, 41, 48, 50, 51, 62],\n",
      " 3: [0, 2, 5, 19, 21, 22, 23, 31, 53, 57, 60],\n",
      " 4: [1, 6, 7, 17, 42, 43, 44, 45, 46, 49],\n",
      " 5: [24, 27, 28, 29, 30, 32, 33, 34, 35, 36, 37, 38, 39, 40, 47]}\n"
     ]
    }
   ],
   "source": [
    "clusters = cluster_texts(documents, inp_cluster)\n",
    "pprint(dict(clusters))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def most_frequent(List): \n",
    "    counter = 0\n",
    "    num = List[0] \n",
    "\n",
    "    for i in List: \n",
    "        curr_frequency = List.count(i) \n",
    "        if(curr_frequency> counter): \n",
    "            counter = curr_frequency \n",
    "            num = i \n",
    "  \n",
    "    return num"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "asosiasi = {}\n",
    "y_true = []\n",
    "y_pred = []\n",
    "\n",
    "for i in range(len(files)):\n",
    "    y_true.append('')\n",
    "    y_pred.append('')\n",
    "    \n",
    "for i in range(len(clusters)):\n",
    "    taglist = []\n",
    "    \n",
    "    for item in clusters[i]:\n",
    "        tag_true = kategori[item]\n",
    "        taglist.append(tag_true)\n",
    "        y_true[item] = tag_true\n",
    "    tagres = most_frequent(taglist)\n",
    "    \n",
    "    for item in clusters[i]:\n",
    "        y_pred[item]=tagres\n",
    "        \n",
    "    asosiasi[tagres] = clusters[i]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'Olahraga': [26, 54, 55, 56, 58, 59, 61],\n",
       " 'Edukasi': [10, 11, 12, 13, 14, 15, 16, 18, 20, 52],\n",
       " 'Bisnis': [3, 4, 8, 9, 25, 41, 48, 50, 51, 62],\n",
       " 'Internasional': [0, 2, 5, 19, 21, 22, 23, 31, 53, 57, 60],\n",
       " 'Nasional': [1, 6, 7, 17, 42, 43, 44, 45, 46, 49],\n",
       " 'Metropolitan': [24, 27, 28, 29, 30, 32, 33, 34, 35, 36, 37, 38, 39, 40, 47]}"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "asosiasi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[53.18941922553139]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'Bisnis': 0,\n",
       " 'Edukasi': 1,\n",
       " 'Internasional': 2,\n",
       " 'Metropolitan': 3,\n",
       " 'Nasional': 4,\n",
       " 'Olahraga': 5}"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "c"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_true = [c[item] for item in y_true]\n",
    "y_pred = [c[item] for item in y_pred]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "def purity_score(y_true, y_pred):\n",
    "    y_voted_labels = np.zeros(y_true.shape)\n",
    "    labels = np.unique(y_true)\n",
    "    ordered_labels = np.arange(labels.shape[0])\n",
    "    for k in range(labels.shape[0]):\n",
    "        y_true[y_true==labels[k]] = ordered_labels[k]\n",
    "    # Update unique labels\n",
    "    labels = np.unique(y_true)\n",
    "    bins = np.concatenate((labels, [np.max(labels)+1]), axis=0)\n",
    "\n",
    "    for cluster in np.unique(y_pred):\n",
    "        hist, _ = np.histogram(y_true[y_pred==cluster], bins=bins)\n",
    "        # Find the most present label in the cluster\n",
    "        winner = np.argmax(hist)\n",
    "        y_voted_labels[y_pred==cluster] = winner\n",
    "    \n",
    "    return accuracy_score(y_true, y_voted_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.6031746031746031"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_true = np.array(y_true)\n",
    "y_pred = np.array(y_pred)\n",
    "purity_score(y_true, y_pred)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
